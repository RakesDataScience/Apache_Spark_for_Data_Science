
4 + year of experience in developing distributed computing Big Data applications using Open Source frameworks like Apache Spark, Apache hadoop,Hive,kafka etc.

Good understanding of Spark Features & Advantages over Map Reduce or Traditional Systems.

Very good hands-on in Spark core,spark sql,spark streaming and Spark machine learning.

Solid Understanding of RDD Operations in Apache Spark i,e Transformations & Actions , Persistence (Caching) ,Accumulators, Broadcast Variables, Optimizing Broadcasts, Converting Between RDD Types.

In depth understanding of Apache spark job execution Components like DAG,lineage graph,Dag Schedular ,Task schedular, Stages and task.

Good understaning Driver,Executor Spark web UI.

Experience in submitting Apache Spark job and mapreduce jobs to YARN.

Experience in exposing Apache Spark as web services.

Worked under direction of data scientist to develop an efficient solution to a predictive analytics problem, testing a number of potential machine learning algorithms of apache spark.

Experience in real time processing using Apache spark and kafka.

Have good understanding of No SQL database like Cassanda and mangodb.

Delivered at three end-to-end Big data analytical based  solutions.


Mirgrated Python Machine learning modules to scalable, high performance and fault-tolerant distributed systems like Apache Spark.

Expericne in Spark SQL UDFs, Hive UDFs, Spark SQL Performance, Performance Tuning

Worked with large data sets from multiple sources utilizing Big Data tools and techniques for the purposes of analysing, providing insight and validating hypotheses

Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo


Experience leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test 

Hands on experience leading delivery through Agile methodologies


Good Expertise in coding in Python,Scala and Java 

Experience in working with automated build and continuous integration systems (Chef, Jenkins, Docker)

Experience managing code on Github

Very experience with working in AWS environment

Good experience in OOPS concepts, design frameworks like Springs, Hibernate, XML, JSON, etc.

Worked on Core Java and J2EE.

Have ability to write SQL Queries, Understanding of RDBMS, Knowledge of Data Modelling Concepts

----------------------------------------------------



Demonstrated experience and mentoring of junior Hadoop developers working with Data Science teams to enable development and deployment of predictive models leveraging various technologies on the Hadoop platform such as R, Python, Spark, H20, SAS etc. strongly desired. Extensive experience with statistics is not required, however this individual will work closely with data scientists and statisticians providing IT support for deployment of data science and predictive applications.


Analyze high-volume, unstructured data sources to build new structured models for product quality assessment





Hands-on proficiency in Spark, Hadoop, HDFS, MapReduce

Experience in Hadoop/HDFS, Apache Spark, Storm, Kafka, MongoDB, Cassandra, Docker/Kubernetes 

Basic understanding of Apache HDFS, Hive, MapReduce, YARN, Spark

Experience in Machine Learning and Data Science

Experience with building stream-processing systems, using solutions such as Spark-Streaming, Storm or Kinesis

Working under direction of data scientist to develop an efficient solution to a predictive analytics problem, testing a number of potential ML solutions

Debugging and generally supervising code production, validation and testing

Expert in Python 2 and 3 programming and machine learning/predictive analytics

Understanding of big data and development best practices

Experience with NoSQL databases, such as HBase, Cassandra, MongoDB

Knowledge of various ETL techniques and frameworks, such as Flume

Experience with various messaging systems, such as Kafka or RabbitMQ

Experience with Big Data ML toolkits, such as SparkML, scikit-learn, H2O, etc.

 Develop algorithms and tools to analyze large data sets consisting of billions of rows
 Develop and deploy machine learning algorithms

 Experience with best practices in software development including agile methods, code review, unit/functional/integration testing, continuous integration/deployment

Portfolio Intelligence Platform 


PIP is an investment eecommendations platform which enables financial and strategy professionals to make better decision on investment plans.

Decisions by unlocking insights from billions of data points with advanced analytics.
With help users are able to find valuable economic insights without having to build complex analytics infrastructure, hunt and
purchase large numbers of individual datasets,
and hire teams of data scientists to analyze them.

PIP collects satellite imagery,web traffic, earnings transcript sentiment, and
other observable data known as “economic
exhaust” or “alternative datasets”.

PIP enables investment
managers to understand, visualize, and optimize
their portfolios by uncovering the portfolio’s
exposure to 50+ relevant market factors using
cutting edge
data science.

Roles:

Worked pysaprk dataframe to read text data,csv data,image data from HDFS, S3 and Hive.
Worked closely data scienctist for building predictive model using Pyspark.
Cleaned input text data using Pyspark ml feature exactions API.
Created features to train algorithms.
Used various algorithms of Pyspark ML API.
Saved mined dataframe to MangoDB
Trained model using historical data stored in HDFS and Amazon S3.
Used Spark Streaming to load the trained model to predict on real time data from kafka.
Stored the result in MongoDB where the web application can pick it up.
Used Apache Zeppelin to vizualization of Big Data.
Fully automated job scheduling, monitoring, and cluster management without human
intervention using webflow.

Tools and Technologies:Amazon EMR, Apache Spark,MangoDB,HDFS, Flask, Hive, Flak, WebFlow,Github.





