
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

# Prepare training documents from a list of (id, text, label) tuples.
training = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)], ["id", "text", "label"])

# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.01)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to training documents.
model = pipeline.fit(training)

# Prepare test documents, which are unlabeled (id, text) tuples.
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "mapreduce spark"),
    (7, "apache hadoop")], ["id", "text"])

# Make predictions on test documents and print columns of interest.
prediction = model.transform(test)
selected = prediction.select("id", "text", "prediction")
for row in selected.collect():
    print(row)

from pyspark.ml.pipeline import Pipeline
from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, StringIndexer, VectorIndexer
df2 = sqlContext.createDataFrame([
    [1,"Here is some text to illustrate pipeline"],
    [2,"and tfidf, which stands for term frequency inverse document frequency"
    ]]).toDF("LineNo","Text")

tok = Tokenizer(inputCol="Text",outputCol="words")
// HashingTF maps a sequence of words to their term frequencies using hashing
// Larger the numFeatures, lower the hashing collision possibility
tf = HashingTF(inputCol="words", outputCol="tf",numFeatures=1000)
// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces weightage of commonly occuring words
idf = IDF(inputCol = "tf",outputCol="tf_idf")
// VectorAssembler merges multiple columns into a single vector column
va = VectorAssembler(inputCols=["tf_idf"],outputCol="features")

//Define pipeline
tfidf_pipeline = Pipeline(stages=[tok,tf,idf,va])
tfidf_pipeline.getStages()

 //Now execute the pipeline 
result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").collect()
[(x[0],x[1]) for x in result]


from pyspark.ml.pipeline import Pipeline
from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, StringIndexer, VectorIndexer
df2 = sqlContext.createDataFrame([
    [1,"Here is some text to illustrate pipeline"],
    [2,"and tfidf, which stands for term frequency inverse document frequency"
    ]]).toDF("LineNo","Text")

tok = Tokenizer(inputCol="Text",outputCol="words")
tf = HashingTF(inputCol="words", outputCol="tf",numFeatures=1000)
idf = IDF(inputCol = "tf",outputCol="tf_idf")
va = VectorAssembler(inputCols=["tf_idf"],outputCol="features")

tfidf_pipeline = Pipeline(stages=[tok,tf,idf,va])
tfidf_pipeline.getStages()

result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").collect()

[(x[0],x[1]) for x in result]



