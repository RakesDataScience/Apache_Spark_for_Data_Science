import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
val raw_data = spark.read.options(Map({"sep" -> "\t"})).csv("mnt/learningspark/SMSSpamCollection")
raw_data.columns
val df = raw_data.select($"_c0".alias("label"),$"_c1".alias("text"))
df.show()
val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
val wordsDataFrame = tokenizer.transform(df)
wordsDataFrame.show()
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover()
  .setInputCol("words")
  .setOutputCol("filtered")
val filtered_words = remover.transform(wordsDataFrame)
filtered_words.show()
import org.apache.spark.ml.feature.StringIndexer
val indexer = new StringIndexer().setInputCol("label").setOutputCol("labelIndex")
val indexed_df = indexer.fit(filtered_words).transform(filtered_words)
indexed_df.show()
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
val hashingTF = new HashingTF()
  .setInputCol("filtered").setOutputCol("rawFeatures").setNumFeatures(10000)
val featurizedData = hashingTF.transform(indexed_df)
featurizedData.show()
val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
val idfModel = idf.fit(featurizedData)
val rescaledData = idfModel.transform(featurizedData)
rescaledData.select("label", "features").show()
val Array(train, test) = rescaledData.randomSplit(Array(0.8, 0.2))
import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.3)
  .setElasticNetParam(0.8).setLabelCol("labelIndex").setFeaturesCol("features")
val lrModel  = lr.fit(train)
val predictions = lrmodel.transform(test)
predictions.select("prediction", "labelIndex", "features").show(5)
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy = evaluator.evaluate(predictions)
println("Test Error = " + (1.0 - accuracy))
val Array(train, test) = df.randomSplit(Array(0.8, 0.2))
import org.apache.spark.ml.Pipeline
val Array(train, test) = df.randomSplit(Array(0.8, 0.2))
val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
val remover = new StopWordsRemover()
  .setInputCol("words")
  .setOutputCol("filtered")
val indexer = new StringIndexer().setInputCol("label").setOutputCol("labelIndex")
val hashingTF = new HashingTF()
  .setInputCol("filtered").setOutputCol("rawFeatures").setNumFeatures(10000)
val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.3)
  .setElasticNetParam(0.8).setLabelCol("labelIndex").setFeaturesCol("features")
val pipeline_lg = new Pipeline()
  .setStages(Array(tokenizer, remover, indexer,hashingTF, idf,lr))
val pipeline_lg_model = pipeline_lg.fit(train)
val pipeline_lg_predictions = pipeline_lg_model.transform(test)
val pipeline_lg_evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val pipeline_lg_accuracy = pipeline_lg_evaluator.evaluate(pipeline_lg_predictions)
println("Test Error = " + (1.0 - pipeline_lg_accuracy))
import org.apache.spark.ml.classification.DecisionTreeClassificationModel
import org.apache.spark.ml.classification.DecisionTreeClassifier
val dt = new DecisionTreeClassifier()
  .setLabelCol("indexedLabel")
  .setFeaturesCol("indexedFeatures").setLabelCol("labelIndex").setFeaturesCol("features")
val pipeline_dt = new Pipeline()
  .setStages(Array(tokenizer, remover, indexer,hashingTF, idf,dt))
val pipeline_dt_model = pipeline_dt.fit(train)
val pipeline_dt_predictions = pipeline_dt_model.transform(test)
val evaluator_dt = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_dt = evaluator_dt.evaluate(pipeline_dt_predictions)
println("Test Error = " + (1.0 - accuracy_dt))
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
val rf = new RandomForestClassifier()
  .setLabelCol("labelIndex")
  .setFeaturesCol("features")
val pipeline_rf = new Pipeline()
  .setStages(Array(tokenizer, remover, indexer,hashingTF, idf,rf))
val pipeline_rf_model = pipeline_rf.fit(train)
val pipeline_rf_predictions = pipeline_rf_model.transform(test)
val evaluator_rf = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_rf = evaluator_rf.evaluate(pipeline_rf_predictions)
println("Test Error = " + (1.0 - accuracy_rf))
import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}
val gbt = new GBTClassifier()
  .setLabelCol("labelIndex")
  .setFeaturesCol("features")
val pipeline_gbt = new Pipeline()
  .setStages(Array(tokenizer, remover, indexer,hashingTF, idf,gbt))
val pipeline_gbt_model = pipeline_gbt.fit(train)
val pipeline_gbt_predictions = pipeline_gbt_model.transform(test)
val evaluator_gbt = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_gbt = evaluator_gbt.evaluate(pipeline_gbt_predictions)
println("Test Error = " + (1.0 - accuracy_gbt))
import org.apache.spark.ml.classification.NaiveBayes
val np = new NaiveBayes().setLabelCol("labelIndex")
  .setFeaturesCol("features")
val pipeline_nb = new Pipeline()
  .setStages(Array(tokenizer, remover, indexer,hashingTF, idf,np))
val pipeline_nb_model = pipeline_nb.fit(train)
val pipeline_nb_predictions = pipeline_nb_model.transform(test)
val evaluator_nb = new MulticlassClassificationEvaluator()
  .setLabelCol("labelIndex")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_nb = evaluator.evaluate(pipeline_nb_predictions)
println("Test set accuracy = " + accuracy_nb)
