val baseRDD = sc.textFile("/mnt/learningspark/crime_data.csv")

val noHeaderRDD = baseRDD.filter { line => ! (line contains "DC_DIST") }

case class CrimeData(dateString: String,
                     timeString: String,
                     offense: String,
                     latitude: String,
                     longitude: String)

val dataRDD = noHeaderRDD.map { line =>
  val cols = line.split(",")
  CrimeData(dateString = cols(10), // DISPATCH_DATE
            timeString = cols(11), // DISPATCH_TIME
            offense    = cols(6),  // TEXT_GENERAL_CODE
            latitude   = cols(7),  // POINT_X
            longitude  = cols(8))  // POINT_Y
}

dataRDD.take(10).foreach(println)

val groupedByOffenseRDD = dataRDD.groupBy { data => data.offense }

groupedByOffenseRDD.take(10)

val offenseCounts = groupedByOffenseRDD.map { case (offense, items) => (offense, items.size) }.collect()

for ((offense, count) <- offenseCounts) {
  println(s"$offense: $count")
}

val BadOffenseRE = """^\d+$""".r

val cleanedRDD = dataRDD.map { data =>
  data.copy(offense = data.offense.replaceAll("\"", "").trim())
}.
filter { data =>
  BadOffenseRE.findFirstIn(data.offense).isEmpty
}

val groupedByOffenseRDD2 = cleanedRDD.groupBy { data => data.offense }

val offenseCounts_2 = groupedByOffenseRDD2.map { case (offense, items) => (offense, items.size) }.collect()

for ((offense, count) <- offenseCounts_2) {
  println(s"$offense: $count")
}

val resultRDD1 = cleanedRDD.filter(_.offense.toLowerCase contains "homicide").map { item => (item.offense, 1) }.reduceByKey(_ + _)

for ((method, count) <- resultRDD1.collect())
println(f"$method%10s $count%d")

cleanedRDD.map(_.dateString).take(30)

import java.text.SimpleDateFormat
val dateFmt = new SimpleDateFormat("yyyy-MM-dd")
val timeFmt = new SimpleDateFormat("HH:mm:ss.S")

import org.apache.spark.sql.types._
import org.apache.spark.sql._
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
import java.sql.Timestamp

def parseDateTime(fmt: SimpleDateFormat, s: String) = {
  try {
    Some(new Timestamp(fmt.parse(s).getTime))
  }
  catch {
    case _: Exception => None
  }
}

case class CrimeData2(date:      Option[java.sql.Timestamp],
                      time:      Option[java.sql.Timestamp],
                      offense:   String,
                      latitude:  String,
                      longitude: String)
                      
                      
val df = noHeaderRDD.map { line =>
  val cols = line.split(",")
  val timeString = cols(11)
  val dateString = cols(10)
  CrimeData2(parseDateTime(dateFmt, dateString),
             parseDateTime(timeFmt, timeString),
             cols(6), 
             cols(7),
             cols(8))
}.
map { data =>
  data.copy(offense = data.offense.replaceAll("\"", "").trim())
}.
filter { data =>
  BadOffenseRE.findFirstIn(data.offense).isEmpty
}.
toDF
df.printSchema()
import org.apache.spark.sql.functions._
df.filter(lower($"offense") like "%homicide%").select(month($"date").as("month"), $"offense").groupBy($"month").count().show()
df.select(month($"date").as("month")).groupBy("month").count().show()
df.select(hour($"time").as("hour"), $"offense").groupBy($"hour").count().show()
