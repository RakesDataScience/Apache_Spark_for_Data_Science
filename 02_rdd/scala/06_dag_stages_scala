val wordList = Array("cat", "cat", "fish", "dog", "fish")
val wordsRDD = sc.parallelize(wordList, 1)
wordsRDD.count()
wordsRDD.partitions.length
wordsRDD.collect()

val wordsRDDWith2Partitions = sc.parallelize(wordList, 2)

wordsRDDWith2Partitions.collect()

wordsRDD.partitions.length

for (p <- wordsRDDWith2Partitions.glom.collect())
  println(p.toSeq)

val datapath = "mnt/learningspark/spark_info.txt"
val spark_infoRDD = sc.textFile(datapath, 4)

spark_infoRDD.count()

spark_infoRDD.partitions.length

spark_infoRDD.getClass

for ((p, i) <- spark_infoRDD.glom.collect().zipWithIndex)
  println(s"$i: ${p.length} item(s)")

sc.textFile(datapath, 4).flatMap(line => line.split(" ")).collect()

sc.textFile(datapath, 4).flatMap(line => line.split(" ")).map(word => (word, 1)).collect()

rdd = sc.textFile(datapath, 4)
words = rdd.flatMap(flatMap(line => line.split(" "))
words_1 = words.map(word => (word, 1))
words_count = words_1.reduceByKey((x, y) => x + y)

words_count.collect()
words_count.count()
words_count.take(2)
words_count.cache()
words_count.collect()
words_count.count()
words_count.take(2)

