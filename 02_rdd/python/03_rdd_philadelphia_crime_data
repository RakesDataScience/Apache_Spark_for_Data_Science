from collections import namedtuple
from pprint import pprint
#Reading Data
rdd = sc.textFile("/mnt/learningspark/philadelphia-crime-data-2015-ytd.csv")
rdd.take(10)
print "Count of lines: {0}".format(rdd.count())
totalChars = rdd.map(lambda line: len(line)).reduce(lambda a, b: a + b)
print "Count of characters : {0}".format(totalChars)
#No Header
no_header_rdd = rdd.filter(lambda line: 'DC_DIST' not in line)
no_header_rdd.take(10)
from collections import namedtuple
CrimeData = namedtuple('CrimeData', ['date_string', 'time_string', 'offense', 'latitude', 'longitude'])

def map_line(line):
  cols = line.split(",")
  return CrimeData(date_string=cols[10], time_string=cols[11], offense=cols[6], latitude=cols[7], longitude=cols[8])

data_rdd = no_header_rdd.map(map_line)

print data_rdd.take(10)

data_rdd.map(lambda data: data.offense).distinct().collect()

data_rdd.map(lambda data: data.offense).distinct().count()


import re
BAD_OFFENSE_RE = re.compile(r'^\d+$')

def clean_offense(d):
  d = CrimeData(date_string=d.date_string, 
                time_string=d.time_string,
                offense=d.offense.replace('"', '').strip(),
                latitude=d.latitude,
                longitude=d.longitude)
  return d
  
cleaned_rdd = data_rdd.map(clean_offense).filter(lambda d: BAD_OFFENSE_RE.search(d.offense) is None)

cleaned_rdd.map(lambda data: data.offense).distinct().collect()
cleaned_rdd.map(lambda data: data.offense).distinct().count()

grouped_by_offense_rdd = cleaned_rdd.groupBy(lambda data: data.offense)

grouped_by_offense_rdd.map(lambda x : (x[0], list(x[1]))).take(2)

grouped_by_offense_rdd.keys().take(2)

grouped_by_offense_rdd.values().map(lambda x : list(x)).take(1)

offense_counts = grouped_by_offense_rdd.map(lambda g: (g[0], len(g[1]))).collect()

for offense, count in offense_counts:
  print offense,"and its count is =", count
  
from datetime import datetime
date_format = "%Y-%m-%d"
time_format = "%H:%M:%S"


def parse_date_time(fmt, s, split_on=None):
  if split_on:
    s = s.split(split_on)[0]
  try:
    return datetime.strptime(s, fmt)
  except:
    return None
    

from pyspark.sql.types import *
from decimal import Decimal

CrimeData2 = namedtuple('CrimeData2', ['date', 'time', 'offense', 'latitude', 'longitude'])

def parse_date_time(fmt, s, split_on=None):
  if split_on:
    s = s.split(split_on)[0]
  try:
    return datetime.strptime(s, fmt)
  except:
    return None

def make_row(line):
  cols = line.split(",")
  return CrimeData2(date      = parse_date_time(date_format, cols[10]),
                    time      = parse_date_time(time_format, cols[11], split_on="."),
                    offense   = cols[6],
                    latitude  = cols[7],
                    longitude = cols[8])

def clean_offense2(d):
  d = CrimeData2(date=d.date, 
                 time=d.time,
                 offense=d.offense.replace('"', '').strip(),
                 latitude=d.latitude,
                 longitude=d.longitude)
  return d
  
  
no_header_rdd.take(5)

final_rdd = no_header_rdd.map(make_row).map(clean_offense2).filter(lambda d: BAD_OFFENSE_RE.search(d.offense) is None)
final_rdd.take(5)
final_rdd.map(lambda x :x.date).take(2)
final_rdd.map(lambda x :(x.date).month).take(5)

month_rdd = final_rdd.groupBy(lambda data: (data.date).month)
month_rdd_count = month_rdd.map(lambda g: (g[0], len(g[1]))).collect()

month_rdd_count

month_offense = final_rdd.filter(lambda d: "homicide" in d.offense.lower()).map(lambda data: ((data.date).month,data.offense)).groupByKey()
month_offense_count = month_offense.map(lambda g: (g[0], len(g[1]))).collect()
month_offense_count

hour_offense = final_rdd.map(lambda data: ((data.time).hour,data.offense)).groupByKey()
hour_offense_count = hour_offense.map(lambda g: (g[0], len(g[1]))).collect()
hour_offense_count
