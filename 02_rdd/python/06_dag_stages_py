datapath = "mnt/learningspark/spark_info.txt"

sc.textFile(datapath, 2).collect()

spark_info_rdd = sc.textFile(datapath, 4)

spark_info_rdd.count()

spark_info_rdd.getNumPartitions()

print 'type of spark_info_rdd: {0}'.format(type(spark_info_rdd))

glomDemo = sc.parallelize([1,2,3,4,5,6,7,8,9], 3)

glomDemo.glom().collect()

for (p, i) in glomDemo.glom().zipWithIndex().collect():
  print '%d: %s items(s)' % (i, p)
  

for (p, i) in spark_info_rdd.glom().zipWithIndex().collect():
  print '%d: %s items(s)' % (i, p)

sc.textFile(datapath, 4).flatMap(lambda x: x.split(' ')).collect()

rdd = sc.textFile(datapath).flatMap(lambda x: x.split(' ')).map(lambda s: (s, 1)).collect()

rdd = sc.textFile(datapath)
words = rdd.flatMap(lambda x: x.split(' '))
work_1 = words.map(lambda s: (s, 1))
word_count = work_1.reduceByKey(lambda x, y: x + y)

word_count.collect()
word_count.count()
word_count.cache()
word_count.count()
word_count.take(2)
work_1.cache()
word_count.cache()
word_count.count()
word_count.take(2)
